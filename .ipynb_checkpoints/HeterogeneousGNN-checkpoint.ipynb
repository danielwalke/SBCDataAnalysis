{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18b73dd",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c929aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Assessable data are 528101 cases and 1015074 CBCs\n",
      "Control data are 527038 cases and 1013548 CBCs\n",
      "Sepsis data are 1488 cases and 1526 CBCs\n",
      "$$$$$$$$$$$$$$$$$$$$\n",
      "Testing: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 365794, Sepsis: 490\n",
      "Assessable data are 180494 cases and 366284 CBCs\n",
      "Control data are 180157 cases and 365794 CBCs\n",
      "Sepsis data are 472 cases and 490 CBCs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 437629, Sepsis: 448\n",
      "Assessable data are 157922 cases and 438077 CBCs\n",
      "Control data are 180157 cases and 437629 CBCs\n",
      "Sepsis data are 438 cases and 448 CBCs\n"
     ]
    }
   ],
   "source": [
    "from dataAnalysis.DataAnalysis import DataAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"extdata/sbcdata.csv\", header=0)\n",
    "data_analysis = DataAnalysis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94724e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "y_train = torch.tensor(data_analysis.get_y_train(), dtype=torch.long)\n",
    "X_train = torch.tensor(data_analysis.get_X_train(), dtype=torch.float)\n",
    "\n",
    "y_test = torch.tensor(data_analysis.get_y_test(), dtype=torch.long)\n",
    "X_test = torch.tensor(data_analysis.get_X_test(), dtype=torch.float)\n",
    "\n",
    "y_gw_test = torch.tensor(data_analysis.get_y_gw(), dtype=torch.long)\n",
    "X_gw_test = torch.tensor(data_analysis.get_X_gw(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e6eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = torch.concat((y_train, y_test, y_gw_test))\n",
    "X_all = torch.concat((X_train, X_test, X_gw_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f90b6",
   "metadata": {},
   "source": [
    "## Train/Validation/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_indices_like(tensor):\n",
    "    return torch.ones((tensor.shape[0])).type(torch.bool)\n",
    "\n",
    "def false_indices_like(tensor):\n",
    "    return torch.zeros((tensor.shape[0])).type(torch.bool)\n",
    "\n",
    "def split(train_features):\n",
    "    tensor = true_indices_like(train_features)\n",
    "    max_index = round(tensor.shape[0] * 0.8)\n",
    "    train = torch.zeros(tensor.shape[0])\n",
    "    train[:max_index] = 1\n",
    "    \n",
    "    val = torch.zeros(tensor.shape[0])\n",
    "    val[max_index:] = 1\n",
    "    return{\n",
    "        \"train\": train.type(torch.bool),\n",
    "        \"val\":val.type(torch.bool)\n",
    "    }\n",
    "train_data = split(X_train)\n",
    "\n",
    "train_mask = torch.concat((train_data[\"train\"], false_indices_like(X_test), false_indices_like(X_gw_test)))\n",
    "val_mask = torch.concat((train_data[\"val\"], false_indices_like(X_test), false_indices_like(X_gw_test)))\n",
    "test_l_mask = torch.concat((false_indices_like(X_train), true_indices_like(X_test), false_indices_like(X_gw_test)))\n",
    "test_gw_mask = torch.concat((false_indices_like(X_train), false_indices_like(X_test), true_indices_like(X_gw_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360310be",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c59b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataAnalysis.Constants import *\n",
    "\n",
    "def to_tensor(df):\n",
    "    return torch.Tensor(list(df.values))\n",
    "\n",
    "def get_quantil_tensor():\n",
    "    number_of_quantiles = 10\n",
    "    q = torch.arange(0, 1, 1/number_of_quantiles)\n",
    "    q = torch.Tensor([0.025,0.05, 0.1, 0.2, 0.35, 0.5, 0.65, 0.8, 0.9, 0.95, 0.975, 1])\n",
    "    return q\n",
    "\n",
    "def get_quantiles(tensor):\n",
    "    q = get_quantil_tensor() \n",
    "    return torch.quantile(tensor, q)\n",
    "\n",
    "def normalize(tensor):\n",
    "    mean = torch.mean(tensor, dim = 0)\n",
    "    std = torch.std(tensor, dim = 0)\n",
    "    mean_diff = tensor - mean\n",
    "    return mean_diff / std\n",
    "\n",
    "def get_quantile_indices(tensor, quantiles):\n",
    "    quantile_indices = []\n",
    "    all_indices = torch.Tensor([])\n",
    "    prev_quantile = -1e-4\n",
    "    indices_control = torch.arange(0, tensor.shape[0])\n",
    "    for i in range(quantiles.nelement()):\n",
    "        indices_u = (tensor > prev_quantile).nonzero(as_tuple=True)[0] # (tensor > prev_quantile and tensor <= quantiles[i]).nonzero(as_tuple=True)[0]\n",
    "        indices_o = (tensor <= quantiles[i]).nonzero(as_tuple=True)[0]\n",
    "        indices = torch.from_numpy(np.intersect1d(indices_u, indices_o))\n",
    "        quantile_indices.append(indices)\n",
    "        prev_quantile = quantiles[i]\n",
    "    return quantile_indices\n",
    "\n",
    "\n",
    "def create_node_features(node_type, quantiles):\n",
    "    nodes_features = []\n",
    "    prev_quantile = torch.Tensor([0])\n",
    "    for i in range(quantiles.nelement()):\n",
    "        node_features = [prev_quantile.item(), quantiles[i].item(), get_quantil_tensor()[i].item()]\n",
    "        prev_quantile = quantiles[i]\n",
    "        nodes_features.append(node_features)\n",
    "    return torch.tensor(nodes_features)\n",
    "\n",
    "def create_edge_features_to_patient(node_type, quantile_indices):\n",
    "    source_edge_list = None\n",
    "    target_edge_list = None\n",
    "    for i in range(len(quantile_indices)):\n",
    "        target_edges = torch.ones((quantile_indices[i].nelement())) * i\n",
    "        source_edges = quantile_indices[i]\n",
    "        source_edge_list = source_edges if source_edge_list is None else torch.concat((source_edge_list, source_edges))\n",
    "        target_edge_list = target_edges if target_edge_list is None else torch.concat((target_edge_list, target_edges))\n",
    "    return torch.stack([source_edge_list, target_edge_list]).type(torch.long)\n",
    "\n",
    "def add_features_and_edges(graph):\n",
    "    for i, feature_name in enumerate(FEATURES):\n",
    "        if feature_name == SEX_CATEGORY_COLUMN_NAME or feature_name == AGE_COLUMN_NAME:\n",
    "            continue\n",
    "        feature_vector = graph[PATIENT_NAME].x[:, i]\n",
    "        node_quantiles = get_quantiles(feature_vector)\n",
    "        quantile_indices = get_quantile_indices(feature_vector, node_quantiles)\n",
    "        graph[feature_name].x = create_node_features(feature_name, node_quantiles)\n",
    "        graph[PATIENT_NAME, EDGE_TYPE, feature_name].edge_index = create_edge_features_to_patient(feature_name, quantile_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8b2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def normalize(tensor):\n",
    "    mean = torch.mean(tensor, dim = 0)\n",
    "    std = torch.std(tensor, dim = 0)\n",
    "    mean_diff = tensor - mean\n",
    "    return mean_diff / std\n",
    "\n",
    "graph = HeteroData()\n",
    "graph[PATIENT_NAME].x = X_all\n",
    "add_features_and_edges(graph)\n",
    "graph[PATIENT_NAME].y = y_all\n",
    "graph[PATIENT_NAME].train_mask = train_mask\n",
    "graph[PATIENT_NAME].val_mask = val_mask\n",
    "graph[PATIENT_NAME].test_l_mask = test_l_mask\n",
    "graph[PATIENT_NAME].test_gw_mask = test_gw_mask\n",
    "graph = T.ToUndirected()(graph)\n",
    "graph[PATIENT_NAME].x = normalize(graph[PATIENT_NAME].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e380f8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mPATIENT\u001b[0m={\n",
       "    x=[1819435, 7],\n",
       "    y=[1819435],\n",
       "    train_mask=[1819435],\n",
       "    val_mask=[1819435],\n",
       "    test_l_mask=[1819435],\n",
       "    test_gw_mask=[1819435]\n",
       "  },\n",
       "  \u001b[1mHGB\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mWBC\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mRBC\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mMCV\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mPLT\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1m(PATIENT, HAS, HGB)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, WBC)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, RBC)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, MCV)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, PLT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(HGB, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(WBC, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(RBC, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(MCV, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PLT, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] }\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e41cf",
   "metadata": {},
   "source": [
    "### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e45295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv, to_hetero,Linear, HeteroConv, SAGEConv\n",
    "from torch_geometric.nn.conv import HANConv\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, out_channels = 1):\n",
    "        hidden_channels = 128\n",
    "        out_channels = 1\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "#         self.lin = Linear(-1, 16)\n",
    "        self.conv1 = SAGEConv((-1,-1), hidden_channels, normalize=True,aggr = \"mean\",root_weight = True)\n",
    "        self.conv_end = SAGEConv(hidden_channels, out_channels, aggr = \"mean\", root_weight = True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv_end(x, edge_index)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6854b",
   "metadata": {},
   "source": [
    "## Test other GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdaa2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HANConv, HGTConv\n",
    "import torch\n",
    "from dataAnalysis.Constants import FEATURES\n",
    "\n",
    "\n",
    "class HetGraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, metadata,node_types, hidden_dim = 128, out_channels = 1):\n",
    "        super(HetGraphNeuralNetwork, self).__init__()\n",
    "                 \n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in node_types:\n",
    "            self.lin_dict[node_type] = Linear(-1, hidden_dim)\n",
    "        self.lin_end = Linear(-1, out_channels)\n",
    "#         self.conv1 = HANConv(-1,hidden_dim, metadata)\n",
    "#         self.conv_end = HANConv(hidden_dim, hidden_dim, metadata)\n",
    "        self.conv1 = HANConv(-1,hidden_dim, metadata)\n",
    "        self.conv_end = HANConv(hidden_dim, hidden_dim, metadata)\n",
    "\n",
    "\n",
    "    def forward(self, x_dict, edge_index):\n",
    "        for node_type, x in x_dict.items():\n",
    "            x_dict[node_type] = self.lin_dict[node_type](x).relu_()\n",
    "        x_dict = self.conv1(x_dict, edge_index)\n",
    "        x_dict = self.conv_end(x_dict, edge_index)\n",
    "        x_dict[PATIENT_NAME] = self.lin_end(x_dict[PATIENT_NAME])\n",
    "        \n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b909594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv,GATv2Conv, GINConv, global_add_pool, Linear\n",
    "import torch\n",
    "from dataAnalysis.Constants import FEATURES\n",
    "from torch.nn import ReLU, Sequential\n",
    "from torch.nn import BatchNorm1d as BatchNorm\n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.lin = Linear(-1, hidden_dim)\n",
    "        mlp_1 = Sequential(\n",
    "            Linear(hidden_dim, 2 * hidden_dim),\n",
    "            BatchNorm(2 * hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(2 * hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.conv_1 = GINConv(mlp_1, train_eps=True).jittable()\n",
    "        mlp_end = Sequential(\n",
    "            Linear(hidden_dim, 2 * hidden_dim),\n",
    "            BatchNorm(2 * hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(2 * hidden_dim, out_channels),\n",
    "        )\n",
    "        self.conv_end = GINConv(mlp_end, train_eps=True).jittable()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin(x)\n",
    "        x = self.conv_1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv_end(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfcec79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv, Linear, GATConv, GATv2Conv\n",
    "import torch\n",
    "from dataAnalysis.Constants import FEATURES\n",
    "from torch.nn import ReLU, Sequential\n",
    "from torch.nn import BatchNorm1d as BatchNorm\n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv((-1, -1), hidden_dim, add_self_loops=False, heads=1)\n",
    "#         self.lin1 = Linear(-1, hidden_dim)\n",
    "        self.conv2 = GATv2Conv((-1, -1), out_channels, add_self_loops=False)\n",
    "#         self.lin2 = Linear(-1, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)# + self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)# + self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e56db",
   "metadata": {},
   "source": [
    "## Shift data to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97de16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shifted to the device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "graph = graph.to(device)\n",
    "\n",
    "sepsis_cases = torch.count_nonzero(graph[PATIENT_NAME].y[train_mask])\n",
    "control_cases = graph[PATIENT_NAME].y[train_mask].size(dim=0) - sepsis_cases\n",
    "WEIGHT = control_cases / (sepsis_cases + 1e-10)\n",
    "WEIGHT = WEIGHT.to(device)\n",
    "\n",
    "print(\"Data shifted to the device \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a6b14",
   "metadata": {},
   "source": [
    "## Model-Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96602f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.nn import to_hetero\n",
    "\n",
    "class ModelWrapper():\n",
    "    def __init__(self, graph):\n",
    "        self.LEARNING_RATE = 3e-4\n",
    "        self.MAX_EPOCHS = 40000\n",
    "        \n",
    "        self.graph = graph\n",
    "        model = HetGraphNeuralNetwork(graph.metadata(),graph.node_types, hidden_dim = 64, out_channels=1) \n",
    "#         model = GraphNeuralNetwork(hidden_dim = 128, out_channels=1)         \n",
    "#         model = to_hetero(model, graph.metadata(), aggr='sum')\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LEARNING_RATE,betas=(0.9, 0.999), eps=1e-08)\n",
    "        \n",
    "        self.last_loss = 0\n",
    "        self.increased_loss = 0\n",
    "        self.BREAKING_THRESHOLD = 10    \n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def validate(self):\n",
    "        with torch.inference_mode():\n",
    "            self.model.eval()\n",
    "            out = self.model(self.graph.x_dict, self.graph.edge_index_dict)[PATIENT_NAME]\n",
    "            loss = F.binary_cross_entropy_with_logits(torch.squeeze(out[val_mask]), self.graph[PATIENT_NAME].y[val_mask].type(torch.float32),\n",
    "                                                      pos_weight=WEIGHT)\n",
    "            self.val_loss.append(loss.item())\n",
    "            if loss.item() > self.last_loss:\n",
    "                self.increased_loss += 1\n",
    "            else:\n",
    "                self.increased_loss = 0\n",
    "            self.last_loss = loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.MAX_EPOCHS):\n",
    "#             print(epoch)\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(self.graph.x_dict, self.graph.edge_index_dict)[PATIENT_NAME]\n",
    "            loss = F.binary_cross_entropy_with_logits(torch.squeeze(out[train_mask]), self.graph[PATIENT_NAME].y[train_mask].type(torch.float32),\n",
    "                                                      pos_weight=WEIGHT)\n",
    "            self.train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.validate() \n",
    "\n",
    "            if self.increased_loss >= self.BREAKING_THRESHOLD:\n",
    "                print(f\"Breaked at {str(epoch)}\")\n",
    "                break\n",
    "            if epoch % 100 == 3:\n",
    "                def predict_proba(graph, mask):\n",
    "                    with torch.inference_mode():\n",
    "                        model.eval()\n",
    "                        logits = model(graph.x_dict, graph.edge_index_dict)[PATIENT_NAME]\n",
    "                        scores = torch.sigmoid(torch.squeeze(logits[mask]))\n",
    "                        scores = torch.unsqueeze(scores, 0)\n",
    "                        proba_predict = torch.concat((1- scores, scores), dim = 0)\n",
    "                    return torch.transpose(proba_predict, 0, 1)\n",
    "\n",
    "                def predict(graph, mask):\n",
    "                    pred = torch.round(predict_proba(graph, mask)[:, 1])\n",
    "                    return pred\n",
    "                model = self.model\n",
    "                graph = self.graph\n",
    "                model.predict_proba = predict_proba\n",
    "                model.predict = predict\n",
    "                from dataAnalysis.Metrics import Evaluation\n",
    "\n",
    "                graph = graph.cpu()\n",
    "                model = model.cpu()\n",
    "                evaluation = Evaluation(y_test, y_gw_test, X_test, X_gw_test)\n",
    "                evaluation.set_test_args([graph, test_l_mask])\n",
    "                evaluation.set_gw_args([graph, test_gw_mask])\n",
    "                print(evaluation.get_df_metrics(model))\n",
    "                model = model.to(device)\n",
    "                graph = graph.to(device)\n",
    "                \n",
    "            \n",
    "    def get_model(self):\n",
    "        return self.model    \n",
    "    \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.epochs, self.train_loss, 'g', label='Training loss')\n",
    "        plt.plot(self.epochs, self.val_loss, 'y', label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c5fc9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
      "0     Leipzig  0.021488  0.798517  0.446833   0.005766  0.609062  0.004751\n",
      "1  Greifswald  0.020550  0.818671  0.452529   0.004811  0.634238  0.003914\n",
      "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
      "0     Leipzig  0.038457  0.730474  0.425665   0.007260  0.800036  0.006720\n",
      "1  Greifswald  0.028154  0.687493  0.409672   0.004697  0.770660  0.003873\n",
      "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
      "0     Leipzig  0.044224  0.778418  0.441971   0.008672  0.823916  0.007492\n",
      "1  Greifswald  0.033878  0.736994  0.427050   0.005644  0.800293  0.004425\n",
      "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
      "0     Leipzig  0.045896  0.791850  0.446430   0.009149  0.825157  0.007525\n",
      "1  Greifswald  0.034220  0.749718  0.431321   0.005803  0.801854  0.004407\n",
      "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
      "0     Leipzig  0.044334  0.780231  0.442567   0.008719  0.827275  0.007424\n",
      "1  Greifswald  0.033431  0.733042  0.425694   0.005561  0.802925  0.004385\n",
      "Breaked at 502\n"
     ]
    }
   ],
   "source": [
    "model_wrapper = ModelWrapper(graph)\n",
    "model_wrapper.train()\n",
    "model = model_wrapper.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab05a48",
   "metadata": {},
   "source": [
    "## Error evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc76c44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaked at 1131\n",
      "947.5445964336395\n",
      "Breaked at 1096\n",
      "913.2297537326813\n",
      "Breaked at 1364\n",
      "1136.0587239265442\n",
      "Breaked at 965\n",
      "805.4357764720917\n",
      "Breaked at 1095\n",
      "917.1277828216553\n",
      "Breaked at 1001\n",
      "840.9021735191345\n",
      "Breaked at 1388\n",
      "1164.1618280410767\n",
      "Breaked at 989\n",
      "825.4386069774628\n",
      "Breaked at 1131\n",
      "949.9604353904724\n",
      "Breaked at 1464\n",
      "1219.9765787124634\n",
      "Breaked at 943\n",
      "786.9116775989532\n",
      "Breaked at 1652\n",
      "1376.030599117279\n",
      "Breaked at 1384\n",
      "1152.5390617847443\n",
      "Breaked at 991\n",
      "825.8903501033783\n",
      "Breaked at 1213\n",
      "1009.9937398433685\n",
      "Breaked at 964\n",
      "802.9429016113281\n",
      "Breaked at 1031\n",
      "859.16450548172\n",
      "Breaked at 1025\n",
      "853.8096330165863\n",
      "Breaked at 936\n",
      "780.1196472644806\n",
      "Breaked at 1135\n",
      "945.6127305030823\n",
      "Breaked at 1087\n",
      "905.8909907341003\n",
      "Breaked at 1181\n",
      "984.325790643692\n",
      "Breaked at 1162\n",
      "967.2767231464386\n",
      "Breaked at 980\n",
      "816.659102678299\n",
      "Breaked at 1347\n",
      "1121.822916507721\n",
      "Breaked at 1425\n",
      "1187.1376793384552\n",
      "Breaked at 1307\n",
      "1089.5560595989227\n",
      "Breaked at 1251\n",
      "1042.036738395691\n",
      "Breaked at 1228\n",
      "1022.8482599258423\n",
      "Breaked at 1133\n",
      "943.7413454055786\n",
      "Breaked at 1354\n",
      "1118.7746245861053\n",
      "Breaked at 1130\n",
      "926.1898112297058\n",
      "Breaked at 1274\n",
      "1043.829910516739\n",
      "Breaked at 1641\n",
      "1344.7931470870972\n",
      "Breaked at 1078\n",
      "883.3334491252899\n",
      "Breaked at 1279\n",
      "1048.374363899231\n",
      "Breaked at 971\n",
      "795.4531862735748\n",
      "Breaked at 1059\n",
      "868.015462398529\n",
      "Breaked at 1201\n",
      "985.3459088802338\n",
      "Breaked at 1164\n",
      "953.7599806785583\n",
      "Breaked at 1071\n",
      "877.9327223300934\n",
      "Breaked at 1023\n",
      "838.1124091148376\n",
      "Breaked at 1214\n",
      "994.7078523635864\n",
      "Breaked at 1191\n",
      "976.3730697631836\n",
      "Breaked at 1410\n",
      "1155.2183125019073\n",
      "Breaked at 1175\n",
      "962.7610559463501\n",
      "Breaked at 1250\n",
      "1023.824700832367\n",
      "Breaked at 1200\n",
      "983.5010421276093\n",
      "Breaked at 1047\n",
      "858.1719574928284\n",
      "Breaked at 1363\n",
      "1116.1970744132996\n",
      "Breaked at 1294\n",
      "1060.3868999481201\n",
      "Breaked at 1388\n",
      "1137.483668088913\n",
      "Breaked at 1379\n",
      "1131.0099217891693\n",
      "Breaked at 1395\n",
      "1145.163678407669\n",
      "Breaked at 985\n",
      "807.4596560001373\n",
      "Breaked at 1032\n",
      "846.5850737094879\n",
      "Breaked at 1013\n",
      "829.883599281311\n",
      "Breaked at 1161\n",
      "951.9763820171356\n",
      "Breaked at 1348\n",
      "1104.3075635433197\n",
      "Breaked at 1198\n",
      "982.8461337089539\n",
      "Breaked at 1417\n",
      "1161.4278542995453\n",
      "Breaked at 1101\n",
      "903.2215566635132\n",
      "Breaked at 1046\n",
      "858.9102640151978\n",
      "Breaked at 1161\n",
      "950.9188551902771\n",
      "Breaked at 1309\n",
      "1073.3049914836884\n",
      "Breaked at 1119\n",
      "917.1042513847351\n",
      "Breaked at 1203\n",
      "986.0824966430664\n",
      "Breaked at 1256\n",
      "1029.417720079422\n",
      "Breaked at 1223\n",
      "1001.7028732299805\n",
      "Breaked at 1190\n",
      "975.1359796524048\n",
      "Breaked at 1514\n",
      "1239.7549827098846\n",
      "Breaked at 1020\n",
      "836.680321931839\n",
      "Breaked at 1025\n",
      "840.3977899551392\n",
      "Breaked at 1203\n",
      "985.9417796134949\n",
      "Breaked at 1086\n",
      "890.4405601024628\n",
      "Breaked at 1513\n",
      "1239.6096670627594\n",
      "Breaked at 1093\n",
      "896.2207989692688\n",
      "Breaked at 1714\n",
      "1406.0548133850098\n",
      "Breaked at 1098\n",
      "900.6980400085449\n",
      "Breaked at 1077\n",
      "882.8483877182007\n",
      "Breaked at 1368\n",
      "1122.601500749588\n",
      "Breaked at 1083\n",
      "889.8222749233246\n",
      "Breaked at 1207\n",
      "991.1509954929352\n",
      "Breaked at 1096\n",
      "898.9094033241272\n",
      "Breaked at 1064\n",
      "872.73872590065\n",
      "Breaked at 1197\n",
      "980.4503262042999\n",
      "Breaked at 1235\n",
      "1011.8983659744263\n",
      "Breaked at 1090\n",
      "893.5440835952759\n",
      "Breaked at 1215\n",
      "994.5078098773956\n",
      "Breaked at 1182\n",
      "969.2068495750427\n",
      "Breaked at 1126\n",
      "923.0331282615662\n",
      "Breaked at 1047\n",
      "857.81103682518\n",
      "Breaked at 1252\n",
      "1026.0067477226257\n",
      "Breaked at 1236\n",
      "1017.3676981925964\n",
      "Breaked at 1128\n",
      "931.6431231498718\n",
      "Breaked at 1144\n",
      "939.6012759208679\n",
      "Breaked at 1246\n",
      "1021.7063727378845\n",
      "Breaked at 1131\n",
      "926.8947064876556\n",
      "Breaked at 1239\n",
      "1015.5575425624847\n",
      "Breaked at 1103\n",
      "903.7823338508606\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number_of_iterations = 100\n",
    "dataframes = []\n",
    "for i in range(number_of_iterations):\n",
    "    start = time.time()\n",
    "    model_wrapper = ModelWrapper(graph)\n",
    "    model_wrapper.train()\n",
    "    print(time.time() - start)\n",
    "#     model = model_wrapper.get_model()\n",
    "#     df = evaluation.get_df_metrics(model)\n",
    "#     dataframes.append(df)\n",
    "# for df in dataframes:\n",
    "#     print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce472c7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0edee9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(graph, mask):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        logits = model(graph.x_dict, graph.edge_index_dict)[PATIENT_NAME]\n",
    "        scores = torch.sigmoid(torch.squeeze(logits[mask]))\n",
    "        scores = torch.unsqueeze(scores, 0)\n",
    "        proba_predict = torch.concat((1- scores, scores), dim = 0)\n",
    "    return torch.transpose(proba_predict, 0, 1)\n",
    "\n",
    "def predict(graph, mask):\n",
    "    pred = torch.round(predict_proba(graph, mask)[:, 1])\n",
    "    return pred\n",
    "\n",
    "model.predict_proba = predict_proba\n",
    "model.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5da76ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataAnalysis.Metrics import Evaluation\n",
    "\n",
    "graph = graph.cpu()\n",
    "model = model.cpu()\n",
    "evaluation = Evaluation(y_test, y_gw_test, X_test, X_gw_test)\n",
    "evaluation.set_test_args([graph, test_l_mask])\n",
    "evaluation.set_gw_args([graph, test_gw_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc4e1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>F1-Binary</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leipzig</td>\n",
       "      <td>0.045287</td>\n",
       "      <td>0.765949</td>\n",
       "      <td>0.437958</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.834404</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greifswald</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.704440</td>\n",
       "      <td>0.415860</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.802674</td>\n",
       "      <td>0.004449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
       "0     Leipzig  0.045287  0.765949  0.437958   0.008604  0.834404  0.008302\n",
       "1  Greifswald  0.033061  0.704440  0.415860   0.005286  0.802674  0.004449"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation.plot_confusion_matrix(model)\n",
    "evaluation.get_df_metrics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68c854",
   "metadata": {},
   "source": [
    "## Maintain connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n",
      "Still sleeping and waiting for you so you dont have to reconnect everything\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def sleeper(minutes):\n",
    "    for i in range(minutes):\n",
    "        time.sleep(60)\n",
    "        print(\"Still sleeping and waiting for you so you dont have to reconnect everything\")\n",
    "sleeper(60*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5addcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
