{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77186db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "sbc = pd.read_csv(\"extdata/sbcdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba680ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811e3de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def apply(group):\n",
    "#     print(np.any(group[\"Diagnosis\"].str.contains(\"Sepsis\").values))\n",
    "#     if group.shape[0] < 2 or not np.any(group[\"Diagnosis\"].str.contains(\"Sepsis\").values):\n",
    "#         return\n",
    "#     group = group.sort_values(\"Time\")\n",
    "#     plt.plot(group[\"Time\"], group[\"WBC\"])\n",
    "#     plt.show()\n",
    "    return group.shape[0]\n",
    "\n",
    "sbc_grouped = sbc.groupby(\"Id\").apply(apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de071de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_grouped = sbc.groupby(\"Id\").filter(lambda x: x.shape[0] >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f785be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_grouped[sbc_grouped[\"Diagnosis\"] == \"Sepsis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_ones = sbc.groupby(\"Id\").filter(lambda x: x.shape[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_ones_sepsis = sbc_ones.loc[sbc_ones[\"Diagnosis\"] == \"Sepsis\", :]\n",
    "sbc_ones_sepsis_val = sbc_ones_sepsis.loc[sbc_ones[\"Set\"] == \"Validation\", :]\n",
    "sbc_ones_sepsis_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.groupby(\"Id\").apply(lambda x: x.loc[x[\"Diagnosis\"] == \"Sepsis\", :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388d3e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Assessable data are 528101 cases and 1015074 CBCs\n",
      "Control data are 527038 cases and 1013548 CBCs\n",
      "Sepsis data are 1488 cases and 1526 CBCs\n",
      "$$$$$$$$$$$$$$$$$$$$\n",
      "Testing: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 365794, Sepsis: 490\n",
      "Assessable data are 180494 cases and 366284 CBCs\n",
      "Control data are 180157 cases and 365794 CBCs\n",
      "Sepsis data are 472 cases and 490 CBCs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 437629, Sepsis: 448\n",
      "Assessable data are 157922 cases and 438077 CBCs\n",
      "Control data are 180157 cases and 437629 CBCs\n",
      "Sepsis data are 438 cases and 448 CBCs\n"
     ]
    }
   ],
   "source": [
    "from dataAnalysis.DataAnalysis import DataAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"extdata/sbcdata.csv\", header=0)\n",
    "data_analysis = DataAnalysis(data, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73b6feb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataAnalysis.Constants import FEATURES_IN_TABLE\n",
    "import numpy as np\n",
    "\n",
    "train_data = data_analysis.get_training_data()\n",
    "test_data = data_analysis.get_testing_data()\n",
    "test_gw_data = data_analysis.get_gw_testing_data()\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "def get_seq(data):\n",
    "    data[\"Label\"] = data.loc[:, \"Label\"] == \"Sepsis\"\n",
    "    data[\"Sex\"] = data.loc[:, \"Sex\"] == \"W\"\n",
    "    data = data.astype({'Sex': 'int8', \"Label\": \"int8\"})\n",
    "    features = []\n",
    "    labels = []\n",
    "    for identifier, group in data.groupby([\"Id\",\"Center\"]):\n",
    "        sorted_group = group.sort_values(\"Time\", ascending=False)\n",
    "        if sorted_group.shape[0] < WINDOW_SIZE:\n",
    "            first_element = sorted_group.iloc[0, :]\n",
    "            first_features = np.expand_dims(first_element.loc[FEATURES_IN_TABLE].values, 0)\n",
    "            first_seq = np.repeat(first_features, WINDOW_SIZE - sorted_group.shape[0], axis= 0)\n",
    "            second_seq = sorted_group.loc[:, FEATURES_IN_TABLE].values\n",
    "            features.append(np.concatenate((first_seq, second_seq), axis = 0))\n",
    "            labels.append(sorted_group.iloc[-1][\"Label\"])\n",
    "        if sorted_group.shape[0] >= WINDOW_SIZE:\n",
    "            last_five_elements = sorted_group.iloc[-5:, :]\n",
    "            last_five_features = last_five_elements.loc[:, FEATURES_IN_TABLE].values\n",
    "            features.append(last_five_features)\n",
    "            labels.append(sorted_group.iloc[-1][\"Label\"])\n",
    "    return features, labels\n",
    "            \n",
    "    \n",
    "train_features, train_labels = get_seq(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "356f4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, dropout: float = 0.0, max_len: int = 5):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, input_dim, 2) * (-math.log(10000.0) / input_dim))\n",
    "        pe = torch.zeros(max_len, 1, input_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        print(x)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        print(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(input_dim)\n",
    "        encoder_layers = TransformerEncoderLayer(input_dim, num_heads, hidden_dim, 0)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, 2)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18b3df30",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (7) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [5, 1, 7].  Tensor sizes: [5, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m LEARNINLEARNING_RATERATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-2\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFEATURES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparams, lr \u001b[38;5;241m=\u001b[39m LEARNING_RATE)\n\u001b[1;32m     10\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn[35], line 38\u001b[0m, in \u001b[0;36mTransformer_Model.__init__\u001b[0;34m(self, input_dim, hidden_dim, output_dim, num_heads)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m encoder_layers \u001b[38;5;241m=\u001b[39m TransformerEncoderLayer(input_dim, num_heads, hidden_dim, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder \u001b[38;5;241m=\u001b[39m TransformerEncoder(encoder_layers, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, input_dim, dropout, max_len)\u001b[0m\n\u001b[1;32m     14\u001b[0m div_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, input_dim, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10000.0\u001b[39m) \u001b[38;5;241m/\u001b[39m input_dim))\n\u001b[1;32m     15\u001b[0m pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(max_len, \u001b[38;5;241m1\u001b[39m, input_dim)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[1;32m     17\u001b[0m pe[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpe\u001b[39m\u001b[38;5;124m'\u001b[39m, pe)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (7) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [5, 1, 7].  Tensor sizes: [5, 4]"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from dataAnalysis.Constants import FEATURES\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNINLEARNING_RATERATE = 3e-2\n",
    "\n",
    "model = Transformer_Model(input_dim = len(FEATURES),hidden_dim = 128, output_dim = 1, num_heads = 1)\n",
    "optimizer = Adam(model.params, lr = LEARNING_RATE)\n",
    "loss_fn = BCEWithLogitsLoss(pos_weight=torch.tensor(10))\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    out = model(X_train)\n",
    "    loss = loss_fn(out.unsqueeze(1), y_train)\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d4e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_646930/4032326726.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Generate sample time series data\n",
    "data = torch.tensor(train_features, dtype=torch.float32) #np.random.randn(10000, 5, 7)  # Shape: [10000, 5, 7]\n",
    "labels = torch.tensor(train_labels) #np.random.randint(2, size=10000)  # Binary labels\n",
    "\n",
    "# Convert data and labels to PyTorch tensors\n",
    "data_tensor = data #torch.tensor(data, dtype=torch.float32)\n",
    "labels_tensor = labels #torch.tensor(labels)\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "\n",
    "# Define the TransformerEncoder class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, dropout, max_len=5000):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.permute(1, 0, 2)  # Adjust the dimensions for the Transformer\n",
    "        output = self.transformer_encoder(x)\n",
    "        output = output.mean(dim=0)  # Average the encoder outputs over the sequence length\n",
    "        output = self.classifier(output).squeeze(1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 7\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "num_heads = 1\n",
    "dropout = 0.1\n",
    "max_len = data.shape[1]\n",
    "model = TransformerEncoder(input_dim, hidden_dim, num_layers, num_heads, dropout, max_len=max_len).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        inputs, labels = dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Evaluation (optional)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    total_correct = 0\n",
    "    for i in range(len(dataset)):\n",
    "        inputs, labels = dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predicted_labels = (outputs >= 0.5).float()\n",
    "\n",
    "        total_correct += (predicted_labels.squeeze() == labels.squeeze()).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(dataset)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d32ab41f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(60):\n",
    "    print(\"Sleeping\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa773121",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d020295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Assessable data are 528101 cases and 1015074 CBCs\n",
      "Control data are 527038 cases and 1013548 CBCs\n",
      "Sepsis data are 1488 cases and 1526 CBCs\n",
      "$$$$$$$$$$$$$$$$$$$$\n",
      "Testing: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 365794, Sepsis: 490\n",
      "Assessable data are 180494 cases and 366284 CBCs\n",
      "Control data are 180157 cases and 365794 CBCs\n",
      "Sepsis data are 472 cases and 490 CBCs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 437629, Sepsis: 448\n",
      "Assessable data are 157922 cases and 438077 CBCs\n",
      "Control data are 180157 cases and 437629 CBCs\n",
      "Sepsis data are 438 cases and 448 CBCs\n"
     ]
    }
   ],
   "source": [
    "from dataAnalysis.DataAnalysis import DataAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"extdata/sbcdata.csv\", header=0)\n",
    "data_analysis = DataAnalysis(data, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = data_analysis.get_training_data() ## TODO in future merge with test and then split again (train test split via masks or train test split based on timeseries subgraphs)\n",
    "source_edge_index = np.array([])\n",
    "target_edge_index = np.array([])\n",
    "for Id, group in df.groupby(\"Id\"):\n",
    "    indices = group.index\n",
    "    indices = np.expand_dims(indices.values, axis = 1)\n",
    "    target = np.repeat(indices, indices.shape[0], axis = 1)\n",
    "    source = target.transpose().flatten()\n",
    "    target = target.flatten()\n",
    "    source_edge_index = np.concatenate((source_edge_index, source))\n",
    "    target_edge_index = np.concatenate((target_edge_index, target))\n",
    "print(source_edge_index)\n",
    "print(target_edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, cuda\n",
    "\n",
    "df = data_analysis.get_training_data() ## TODO in future merge with test and then split again (train test split via masks or train test split based on timeseries subgraphs)\n",
    "source_edge_index = np.array([])\n",
    "target_edge_index = np.array([])\n",
    "\n",
    "@jit(target_backend='cuda')  \n",
    "def fun(group):\n",
    "    indices = group.index\n",
    "    indices = np.expand_dims(indices.values, axis = 1)\n",
    "    target = np.repeat(indices, indices.shape[0], axis = 1)\n",
    "    source = target.transpose().flatten()\n",
    "    target = target.flatten()\n",
    "    source_edge_index = np.concatenate((source_edge_index, source))\n",
    "    target_edge_index = np.concatenate((target_edge_index, target))\n",
    "\n",
    "for Id, group in df.groupby(\"Id\"):\n",
    "    fun(group)\n",
    "print(source_edge_index)\n",
    "print(target_edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
