{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18b73dd",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431abf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "sys.path.insert(0, \"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c929aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/graph_learning/heterogeneous/../../dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Assessable data are 528101 cases and 1015074 CBCs\n",
      "Control data are 527038 cases and 1013548 CBCs\n",
      "Sepsis data are 1488 cases and 1526 CBCs\n",
      "$$$$$$$$$$$$$$$$$$$$\n",
      "Testing: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/graph_learning/heterogeneous/../../dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 365794, Sepsis: 490\n",
      "Assessable data are 180494 cases and 366284 CBCs\n",
      "Control data are 180157 cases and 365794 CBCs\n",
      "Sepsis data are 472 cases and 490 CBCs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/sbc/graph_learning/heterogeneous/../../dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: 437629, Sepsis: 448\n",
      "Assessable data are 157922 cases and 438077 CBCs\n",
      "Control data are 180157 cases and 437629 CBCs\n",
      "Sepsis data are 438 cases and 448 CBCs\n"
     ]
    }
   ],
   "source": [
    "from dataAnalysis.DataAnalysis import DataAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"../../extdata/sbcdata.csv\", header=0)\n",
    "data_analysis = DataAnalysis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94724e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "y_train = torch.tensor(data_analysis.get_y_train(), dtype=torch.long)\n",
    "X_train = torch.tensor(data_analysis.get_X_train(), dtype=torch.float)\n",
    "\n",
    "y_test = torch.tensor(data_analysis.get_y_test(), dtype=torch.long)\n",
    "X_test = torch.tensor(data_analysis.get_X_test(), dtype=torch.float)\n",
    "\n",
    "y_gw_test = torch.tensor(data_analysis.get_y_gw(), dtype=torch.long)\n",
    "X_gw_test = torch.tensor(data_analysis.get_X_gw(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e6eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = torch.concat((y_train, y_test, y_gw_test))\n",
    "X_all = torch.concat((X_train, X_test, X_gw_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f90b6",
   "metadata": {},
   "source": [
    "## Train/Validation/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_indices_like(tensor):\n",
    "    return torch.ones((tensor.shape[0])).type(torch.bool)\n",
    "\n",
    "def false_indices_like(tensor):\n",
    "    return torch.zeros((tensor.shape[0])).type(torch.bool)\n",
    "\n",
    "def split(train_features):\n",
    "    tensor = true_indices_like(train_features)\n",
    "    max_index = round(tensor.shape[0] * 0.8)\n",
    "    train = torch.zeros(tensor.shape[0])\n",
    "    train[:max_index] = 1\n",
    "    \n",
    "    val = torch.zeros(tensor.shape[0])\n",
    "    val[max_index:] = 1\n",
    "    return{\n",
    "        \"train\": train.type(torch.bool),\n",
    "        \"val\":val.type(torch.bool)\n",
    "    }\n",
    "train_data = split(X_train)\n",
    "\n",
    "train_mask = torch.concat((train_data[\"train\"], false_indices_like(X_test), false_indices_like(X_gw_test)))\n",
    "val_mask = torch.concat((train_data[\"val\"], false_indices_like(X_test), false_indices_like(X_gw_test)))\n",
    "test_l_mask = torch.concat((false_indices_like(X_train), true_indices_like(X_test), false_indices_like(X_gw_test)))\n",
    "test_gw_mask = torch.concat((false_indices_like(X_train), false_indices_like(X_test), true_indices_like(X_gw_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360310be",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c59b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataAnalysis.Constants import *\n",
    "\n",
    "def to_tensor(df):\n",
    "    return torch.Tensor(list(df.values))\n",
    "\n",
    "def get_quantil_tensor():\n",
    "    number_of_quantiles = 10\n",
    "    q = torch.arange(0, 1, 1/number_of_quantiles)\n",
    "    q = torch.Tensor([0.025,0.05, 0.1, 0.2, 0.35, 0.5, 0.65, 0.8, 0.9, 0.95, 0.975, 1])\n",
    "    return q\n",
    "\n",
    "def get_quantiles(tensor):\n",
    "    q = get_quantil_tensor() \n",
    "    return torch.quantile(tensor, q)\n",
    "\n",
    "def normalize(tensor):\n",
    "    mean = torch.mean(tensor, dim = 0)\n",
    "    std = torch.std(tensor, dim = 0)\n",
    "    mean_diff = tensor - mean\n",
    "    return mean_diff / std\n",
    "\n",
    "def get_quantile_indices(tensor, quantiles):\n",
    "    quantile_indices = []\n",
    "    all_indices = torch.Tensor([])\n",
    "    prev_quantile = -1e-4\n",
    "    indices_control = torch.arange(0, tensor.shape[0])\n",
    "    for i in range(quantiles.nelement()):\n",
    "        indices_u = (tensor > prev_quantile).nonzero(as_tuple=True)[0] # (tensor > prev_quantile and tensor <= quantiles[i]).nonzero(as_tuple=True)[0]\n",
    "        indices_o = (tensor <= quantiles[i]).nonzero(as_tuple=True)[0]\n",
    "        indices = torch.from_numpy(np.intersect1d(indices_u, indices_o))\n",
    "        quantile_indices.append(indices)\n",
    "        prev_quantile = quantiles[i]\n",
    "    return quantile_indices\n",
    "\n",
    "\n",
    "def create_node_features(node_type, quantiles):\n",
    "    nodes_features = []\n",
    "    prev_quantile = torch.Tensor([0])\n",
    "    for i in range(quantiles.nelement()):\n",
    "        node_features = [prev_quantile.item(), quantiles[i].item(), get_quantil_tensor()[i].item()]\n",
    "        prev_quantile = quantiles[i]\n",
    "        nodes_features.append(node_features)\n",
    "    return torch.tensor(nodes_features)\n",
    "\n",
    "def create_edge_features_to_patient(node_type, quantile_indices):\n",
    "    source_edge_list = None\n",
    "    target_edge_list = None\n",
    "    for i in range(len(quantile_indices)):\n",
    "        target_edges = torch.ones((quantile_indices[i].nelement())) * i\n",
    "        source_edges = quantile_indices[i]\n",
    "        source_edge_list = source_edges if source_edge_list is None else torch.concat((source_edge_list, source_edges))\n",
    "        target_edge_list = target_edges if target_edge_list is None else torch.concat((target_edge_list, target_edges))\n",
    "    return torch.stack([source_edge_list, target_edge_list]).type(torch.long)\n",
    "\n",
    "def add_features_and_edges(graph):\n",
    "    for i, feature_name in enumerate(FEATURES):\n",
    "        if feature_name not in [HGB_COLUMN_NAME, WBC_COLUMN_NAME, RBC_COLUMN_NAME, MCV_COLUMN_NAME, PLT_COLUMN_NAME]:\n",
    "            continue\n",
    "        feature_vector = graph[PATIENT_NAME].x[:, i]\n",
    "        node_quantiles = get_quantiles(feature_vector)\n",
    "        quantile_indices = get_quantile_indices(feature_vector, node_quantiles)\n",
    "        graph[feature_name].x = create_node_features(feature_name, node_quantiles)\n",
    "        graph[PATIENT_NAME, EDGE_TYPE, feature_name].edge_index = create_edge_features_to_patient(feature_name, quantile_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8b2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def normalize(tensor):\n",
    "    mean = torch.mean(tensor, dim = 0)\n",
    "    std = torch.std(tensor, dim = 0)\n",
    "    mean_diff = tensor - mean\n",
    "    return mean_diff / std\n",
    "\n",
    "graph = HeteroData()\n",
    "graph[PATIENT_NAME].x = X_all\n",
    "add_features_and_edges(graph)\n",
    "graph[PATIENT_NAME].y = y_all\n",
    "graph[PATIENT_NAME].train_mask = train_mask\n",
    "graph[PATIENT_NAME].val_mask = val_mask\n",
    "graph[PATIENT_NAME].test_l_mask = test_l_mask\n",
    "graph[PATIENT_NAME].test_gw_mask = test_gw_mask\n",
    "graph = T.ToUndirected()(graph)\n",
    "graph[PATIENT_NAME].x = normalize(graph[PATIENT_NAME].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e380f8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mPATIENT\u001b[0m={\n",
       "    x=[1819435, 7],\n",
       "    y=[1819435],\n",
       "    train_mask=[1819435],\n",
       "    val_mask=[1819435],\n",
       "    test_l_mask=[1819435],\n",
       "    test_gw_mask=[1819435]\n",
       "  },\n",
       "  \u001b[1mHGB\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mWBC\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mRBC\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mMCV\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mPLT\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1m(PATIENT, HAS, HGB)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, WBC)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, RBC)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, MCV)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, PLT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(HGB, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(WBC, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(RBC, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(MCV, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PLT, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] }\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e41cf",
   "metadata": {},
   "source": [
    "### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcec79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv, Linear, GATConv, GATv2Conv\n",
    "import torch\n",
    "from dataAnalysis.Constants import FEATURES\n",
    "from torch.nn import ReLU, Sequential\n",
    "from torch.nn import BatchNorm1d as BatchNorm\n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv((-1, -1), hidden_dim, add_self_loops=False, heads=2)\n",
    "        self.conv2 = GATv2Conv((-1, -1), out_channels, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e56db",
   "metadata": {},
   "source": [
    "## Shift data to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97de16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shifted to the device cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "graph = graph.to(device)\n",
    "\n",
    "sepsis_cases = torch.count_nonzero(graph[PATIENT_NAME].y[train_mask])\n",
    "control_cases = graph[PATIENT_NAME].y[train_mask].size(dim=0) - sepsis_cases\n",
    "WEIGHT = (control_cases / (sepsis_cases + 1e-10))\n",
    "WEIGHT = WEIGHT.to(device)\n",
    "\n",
    "print(\"Data shifted to the device \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a6b14",
   "metadata": {},
   "source": [
    "## Model-Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96602f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.nn import to_hetero\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class ModelWrapper():\n",
    "    def __init__(self, graph):\n",
    "        self.LEARNING_RATE = 3e-4\n",
    "        self.MAX_EPOCHS = 10000\n",
    "        \n",
    "        self.graph = graph\n",
    "#         model = HetGraphNeuralNetwork(graph.metadata(),graph.node_types, hidden_dim = 128, out_channels=1) \n",
    "        model = GraphNeuralNetwork(hidden_dim = 64, out_channels=1)         \n",
    "        model = to_hetero(model, graph.metadata(), aggr='sum')\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LEARNING_RATE,betas=(0.9, 0.999), eps=1e-08)\n",
    "        \n",
    "        self.last_loss = 0\n",
    "        self.increased_loss = 0\n",
    "        self.BREAKING_THRESHOLD = 10    \n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def validate(self):\n",
    "        with torch.inference_mode():\n",
    "            self.model.eval()\n",
    "            out = self.model(self.graph.x_dict, self.graph.edge_index_dict)[PATIENT_NAME]\n",
    "            loss = F.binary_cross_entropy_with_logits(torch.squeeze(out[val_mask]), self.graph[PATIENT_NAME].y[val_mask].type(torch.float32),\n",
    "                                                      pos_weight=WEIGHT)\n",
    "            self.val_loss.append(loss.item())\n",
    "            if loss.item() > self.last_loss:\n",
    "                self.increased_loss += 1\n",
    "            else:\n",
    "                self.increased_loss = 0\n",
    "            self.last_loss = loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.MAX_EPOCHS)):\n",
    "#             print(epoch)\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(self.graph.x_dict, self.graph.edge_index_dict)[PATIENT_NAME]\n",
    "            loss = F.binary_cross_entropy_with_logits(torch.squeeze(out[train_mask]), self.graph[PATIENT_NAME].y[train_mask].type(torch.float32),\n",
    "                                                      pos_weight=WEIGHT)\n",
    "            self.train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.validate() \n",
    "\n",
    "            if self.increased_loss >= self.BREAKING_THRESHOLD:\n",
    "                print(f\"Breaked at {str(epoch)}\")\n",
    "                break\n",
    "                \n",
    "            \n",
    "    def get_model(self):\n",
    "        return self.model    \n",
    "    \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.epochs, self.train_loss, 'g', label='Training loss')\n",
    "        plt.plot(self.epochs, self.val_loss, 'y', label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5fc9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATv2-2 heads\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf27cc3caa94343b7dc8bff11ea4db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.74 GiB (GPU 2; 47.54 GiB total capacity; 44.01 GiB already allocated; 1.72 GiB free; 45.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGATv2-2 heads\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start )\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model_wrapper\u001b[38;5;241m.\u001b[39mget_model()\n",
      "Cell \u001b[0;32mIn[18], line 41\u001b[0m, in \u001b[0;36mModelWrapper.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m[PATIENT_NAME]\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(torch\u001b[38;5;241m.\u001b[39msqueeze(out[train_mask]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph[PATIENT_NAME]\u001b[38;5;241m.\u001b[39my[train_mask]\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     43\u001b[0m                                           pos_weight\u001b[38;5;241m=\u001b[39mWEIGHT)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/fx/graph_module.py:662\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/fx/graph_module.py:281\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/fx/graph_module.py:271\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.3:29\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     27\u001b[0m conv1__PLT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mPATIENT__HAS__PLT((x__PATIENT, x__PLT), edge_index__PATIENT__HAS__PLT)\n\u001b[1;32m     28\u001b[0m conv1__PATIENT1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mHGB__rev_HAS__PATIENT((x__HGB, x__PATIENT), edge_index__HGB__rev_HAS__PATIENT);  x__HGB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m conv1__PATIENT2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWBC__rev_HAS__PATIENT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx__WBC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx__PATIENT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index__WBC__rev_HAS__PATIENT\u001b[49m\u001b[43m)\u001b[49m;  x__WBC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     30\u001b[0m conv1__PATIENT3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mRBC__rev_HAS__PATIENT((x__RBC, x__PATIENT), edge_index__RBC__rev_HAS__PATIENT);  x__RBC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m conv1__PATIENT4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mMCV__rev_HAS__PATIENT((x__MCV, x__PATIENT), edge_index__MCV__rev_HAS__PATIENT);  x__MCV \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/conv/gatv2_conv.py:250\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:484\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    487\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:608\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    596\u001b[0m               ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m               dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:116\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dim_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()):\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered invalid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:109\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/aggr/basic.py:21\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:155\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segment(x, ptr, reduce\u001b[38;5;241m=\u001b[39mreduce)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     77\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 2; 47.54 GiB total capacity; 44.01 GiB already allocated; 1.72 GiB free; 45.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_wrapper = ModelWrapper(graph)\n",
    "import time \n",
    "print(\"GATv2-2 heads\")\n",
    "start = time.time()\n",
    "model_wrapper.train()\n",
    "print(time.time() - start )\n",
    "model = model_wrapper.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47edb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mPATIENT\u001b[0m={\n",
       "    x=[1819435, 7],\n",
       "    y=[1819435],\n",
       "    train_mask=[1819435],\n",
       "    val_mask=[1819435],\n",
       "    test_l_mask=[1819435],\n",
       "    test_gw_mask=[1819435]\n",
       "  },\n",
       "  \u001b[1mHGB\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mWBC\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mRBC\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mMCV\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1mPLT\u001b[0m={ x=[12, 3] },\n",
       "  \u001b[1m(PATIENT, HAS, HGB)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, WBC)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, RBC)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, MCV)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PATIENT, HAS, PLT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(HGB, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(WBC, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(RBC, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(MCV, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] },\n",
       "  \u001b[1m(PLT, rev_HAS, PATIENT)\u001b[0m={ edge_index=[2, 1819435] }\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab05a48",
   "metadata": {},
   "source": [
    "## Error evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc76c44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9891d803d3491c9761289251a42d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4528789520263672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     evaluation\u001b[38;5;241m.\u001b[39mset_test_args([graph, test_l_mask])\n\u001b[1;32m     36\u001b[0m     evaluation\u001b[38;5;241m.\u001b[39mset_gw_args([graph, test_gw_mask])\n\u001b[0;32m---> 38\u001b[0m     df \u001b[38;5;241m=\u001b[39m evaluation\u001b[38;5;241m.\u001b[39mget_df_metrics(model)\n\u001b[1;32m     39\u001b[0m     dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dataframes:\n",
      "File \u001b[0;32m~/git/sbc/graph_learning/../dataAnalysis/Metrics.py:45\u001b[0m, in \u001b[0;36mEvaluation.get_df_metrics\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_df_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m---> 45\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_args) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_test)\n\u001b[1;32m     46\u001b[0m     test_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_args) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict_proba(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_test)\n\u001b[1;32m     47\u001b[0m     gw_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgw_args) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgw_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_gw_test)\n",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(graph, mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(graph, mask):\n\u001b[0;32m---> 24\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(predict_proba(graph, mask)[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mpredict_proba\u001b[0;34m(graph, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(graph\u001b[38;5;241m.\u001b[39mx_dict, graph\u001b[38;5;241m.\u001b[39medge_index_dict)[PATIENT_NAME]\n\u001b[1;32m     18\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39msqueeze(logits[mask]))\n\u001b[1;32m     19\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(scores, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch/fx/graph_module.py:662\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch/fx/graph_module.py:271\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.5:30\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     28\u001b[0m conv1__PATIENT1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mHGB__rev_HAS__PATIENT((x__HGB, x__PATIENT), edge_index__HGB__rev_HAS__PATIENT);  x__HGB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m conv1__PATIENT2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mWBC__rev_HAS__PATIENT((x__WBC, x__PATIENT), edge_index__WBC__rev_HAS__PATIENT);  x__WBC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m conv1__PATIENT3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mRBC__rev_HAS__PATIENT((x__RBC, x__PATIENT), edge_index__RBC__rev_HAS__PATIENT);  x__RBC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m conv1__PATIENT4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mMCV__rev_HAS__PATIENT((x__MCV, x__PATIENT), edge_index__MCV__rev_HAS__PATIENT);  x__MCV \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m conv1__PATIENT5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mPLT__rev_HAS__PATIENT((x__PLT, x__PATIENT), edge_index__PLT__rev_HAS__PATIENT);  x__PLT \u001b[38;5;241m=\u001b[39m x__PATIENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch_geometric/nn/conv/gatv2_conv.py:250\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39m(x_l, x_r), edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[1;32m    251\u001b[0m                      size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    253\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:467\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 467\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmsg_kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    469\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number_of_iterations = 100\n",
    "dataframes = []\n",
    "for i in range(number_of_iterations):\n",
    "    graph = graph.to(device)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    model_wrapper = ModelWrapper(graph)\n",
    "    model_wrapper.train()\n",
    "    print(time.time() - start)\n",
    "    \n",
    "    def predict_proba(graph, mask):\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "            logits = model(graph.x_dict, graph.edge_index_dict)[PATIENT_NAME]\n",
    "            scores = torch.sigmoid(torch.squeeze(logits[mask]))\n",
    "            scores = torch.unsqueeze(scores, 0)\n",
    "            proba_predict = torch.concat((1- scores, scores), dim = 0)\n",
    "        return torch.transpose(proba_predict, 0, 1)\n",
    "\n",
    "    def predict(graph, mask):\n",
    "        pred = torch.round(predict_proba(graph, mask)[:, 1])\n",
    "        return pred\n",
    "    model = model_wrapper.get_model()\n",
    "    model.predict_proba = predict_proba\n",
    "    model.predict = predict\n",
    "    \n",
    "    from dataAnalysis.Metrics import Evaluation\n",
    "\n",
    "    graph = graph.cpu()\n",
    "    model = model.cpu()\n",
    "    evaluation = Evaluation(y_test, y_gw_test, X_test, X_gw_test)\n",
    "    evaluation.set_test_args([graph, test_l_mask])\n",
    "    evaluation.set_gw_args([graph, test_gw_mask])\n",
    "    \n",
    "    df = evaluation.get_df_metrics(model)\n",
    "    dataframes.append(df)\n",
    "for df in dataframes:\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce472c7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0edee9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(graph, mask):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        logits = model(graph.x_dict, graph.edge_index_dict)[PATIENT_NAME]\n",
    "        scores = torch.sigmoid(torch.squeeze(logits[mask]))\n",
    "        scores = torch.unsqueeze(scores, 0)\n",
    "        proba_predict = torch.concat((1- scores, scores), dim = 0)\n",
    "    return torch.transpose(proba_predict, 0, 1)\n",
    "\n",
    "def predict(graph, mask):\n",
    "    pred = torch.round(predict_proba(graph, mask)[:, 1])\n",
    "    return pred\n",
    "\n",
    "model.predict_proba = predict_proba\n",
    "model.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da76ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataAnalysis.Metrics import Evaluation\n",
    "\n",
    "graph = graph.cpu()\n",
    "model = model.cpu()\n",
    "evaluation = Evaluation(y_test, y_gw_test, X_test, X_gw_test)\n",
    "evaluation.set_test_args([graph, test_l_mask])\n",
    "evaluation.set_gw_args([graph, test_gw_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc4e1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>F1-Binary</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leipzig</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>0.772032</td>\n",
       "      <td>0.440053</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.842227</td>\n",
       "      <td>0.008722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greifswald</td>\n",
       "      <td>0.032649</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.414423</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>0.806684</td>\n",
       "      <td>0.004573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name       MCC  F1-Micro  F1-Macro  F1-Binary     AUROC     AUPRC\n",
       "0     Leipzig  0.046765  0.772032  0.440053   0.008902  0.842227  0.008722\n",
       "1  Greifswald  0.032649  0.700384  0.414423   0.005214  0.806684  0.004573"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation.plot_confusion_matrix(model)\n",
    "evaluation.get_df_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c693bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
