{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a53481da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric import datasets\n",
    "\n",
    "dataset = datasets.Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "190c27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "287c694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "split = RandomNodeSplit(num_test=1000)\n",
    "graph = split(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6111028d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.test_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "563e919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "device\n",
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0bb2f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = GCNConv(dataset.num_node_features, 16, normalize=True)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes, normalize=True)\n",
    "        self.conv1 = GATConv(dataset.num_node_features, 16, heads=5)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.conv2 = GATConv((-1,-1), dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        output = self.conv2(x, edge_index)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fa9eb21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GCN().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfab8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8700, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=graph.y.unique().shape[0]).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    logits = model(graph)\n",
    "    preds = torch.softmax(logits, dim = 1).argmax(dim=1)\n",
    "    correct = (preds[graph.test_mask] == graph.y[graph.test_mask]).sum()\n",
    "    print(accuracy(preds[graph.test_mask], graph.y[graph.test_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bb76eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    SparseTensor,\n",
    "    torch_sparse,\n",
    ")\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils import add_self_loops as add_self_loops_fn\n",
    "from torch_geometric.utils import (\n",
    "    is_torch_sparse_tensor,\n",
    "    scatter,\n",
    "    spmm,\n",
    "    to_edge_index,\n",
    ")\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils.sparse import set_sparse_value\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n",
    "             add_self_loops=True, flow=\"source_to_target\", dtype=None):\n",
    "\n",
    "    fill_value = 2. if improved else 1.\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        assert edge_index.size(0) == edge_index.size(1)\n",
    "\n",
    "        adj_t = edge_index\n",
    "\n",
    "        if not adj_t.has_value():\n",
    "            adj_t = adj_t.fill_value(1., dtype=dtype)\n",
    "        if add_self_loops:\n",
    "            adj_t = torch_sparse.fill_diag(adj_t, fill_value)\n",
    "\n",
    "        deg = torch_sparse.sum(adj_t, dim=1)\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "        adj_t = torch_sparse.mul(adj_t, deg_inv_sqrt.view(-1, 1))\n",
    "        adj_t = torch_sparse.mul(adj_t, deg_inv_sqrt.view(1, -1))\n",
    "\n",
    "        return adj_t\n",
    "\n",
    "    if is_torch_sparse_tensor(edge_index):\n",
    "        assert edge_index.size(0) == edge_index.size(1)\n",
    "\n",
    "        if edge_index.layout == torch.sparse_csc:\n",
    "            raise NotImplementedError(\"Sparse CSC matrices are not yet \"\n",
    "                                      \"supported in 'gcn_norm'\")\n",
    "\n",
    "        adj_t = edge_index\n",
    "        if add_self_loops:\n",
    "            adj_t, _ = add_self_loops_fn(adj_t, None, fill_value, num_nodes)\n",
    "\n",
    "        edge_index, value = to_edge_index(adj_t)\n",
    "        col, row = edge_index[0], edge_index[1]\n",
    "\n",
    "        deg = scatter(value, col, 0, dim_size=num_nodes, reduce='sum')\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "        value = deg_inv_sqrt[row] * value * deg_inv_sqrt[col]\n",
    "\n",
    "        return set_sparse_value(adj_t, value), None\n",
    "\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    if add_self_loops:\n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    idx = col if flow == 'source_to_target' else row\n",
    "    deg = scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n",
    "    deg_inv_sqrt = deg.pow_(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "    edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, agg = \"mean\"):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.lin = nn.Linear(in_features, out_features)\n",
    "        self.init_weights()\n",
    "        self.agg:str = agg\n",
    "\n",
    "    def lift(self, x, edge_index):\n",
    "        source_lift = torch.index_select(x, 0, edge_index[0])\n",
    "        target_lift = torch.index_select(x, 0, edge_index[1])\n",
    "        return source_lift, target_lift\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.lin.weight)\n",
    "        self.lin.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, None, x.shape[0],False, False)\n",
    "        x = self.lin(x)\n",
    "        source_lift, target_lift = self.lift(x, edge_index)\n",
    "        x = torch.scatter_reduce(x, 0,  edge_index[1].repeat(x.shape[1],1).t(), source_lift, reduce=self.agg)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ef63dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0910cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.x = graph.x.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9177447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads = 1, agg = \"mean\"):\n",
    "        super(CustomGATLayer, self).__init__()\n",
    "        assert out_features % heads == 0\n",
    "        \n",
    "        self.lin = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        self.lin_k = nn.Linear(out_features, out_features, bias=False)\n",
    "        self.lin_q = nn.Linear(out_features, out_features, bias=False)\n",
    "        self.lin_v = nn.Linear(out_features, out_features, bias=False)\n",
    "        self.out = out_features\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.agg:str = agg\n",
    "\n",
    "    def lift(self, x, edge_index):\n",
    "        source_lift = torch.index_select(x, 0, edge_index[0])\n",
    "        target_lift = torch.index_select(x, 0, edge_index[1])\n",
    "        return source_lift, target_lift\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.lin.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.lin_k.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.lin_q.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.lin_v.weight)\n",
    "        self.lin.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        b, seq_len, feature_dim = x.size()\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        keys = self.lin_k(x)\n",
    "        queries = self.lin_q(x)\n",
    "        values = self.lin_v(x)\n",
    "        \n",
    "        head_dim = feature_dim // self.heads\n",
    "\n",
    "        keys    = keys.view(b, seq_len, self.heads, head_dim)\n",
    "        queries = queries.view(b, seq_len, self.heads, head_dim)\n",
    "        values  = values.view(b, seq_len, self.heads, head_dim)\n",
    "        \n",
    "        qkv = torch.cat((keys, queries, values), 2)\n",
    "        print(qkv)\n",
    "        x = flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=None, causal=False)\n",
    "        print(x)\n",
    "        source_lift, target_lift = self.lift(x, edge_index)\n",
    "        x = torch.scatter_reduce(x, 0,  edge_index[1].repeat(x.shape[1],1).t(), source_lift, reduce=self.agg)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8a8c1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GNNLayer(dataset.num_node_features, 16).to(device)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.conv2 = GNNLayer(16, dataset.num_classes).to(device)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(x)        \n",
    "        output = self.conv2(x, edge_index)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "295d225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = CustomGATLayer(dataset.num_node_features, 16).to(device)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.conv2 = CustomGATLayer(16, dataset.num_classes).to(device)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(x)        \n",
    "        output = self.conv2(x, edge_index)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "515f4db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 2708, 1, 1433]' is invalid for input of size 43328",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m         correct \u001b[38;5;241m=\u001b[39m (preds[graph\u001b[38;5;241m.\u001b[39mtest_mask] \u001b[38;5;241m==\u001b[39m graph\u001b[38;5;241m.\u001b[39my[graph\u001b[38;5;241m.\u001b[39mtest_mask])\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy(preds[graph\u001b[38;5;241m.\u001b[39mtest_mask],\u001b[38;5;250m \u001b[39mgraph\u001b[38;5;241m.\u001b[39my[graph\u001b[38;5;241m.\u001b[39mtest_mask])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_optim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m test(custom_model)\n",
      "Cell \u001b[0;32mIn[217], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 25\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out[graph\u001b[38;5;241m.\u001b[39mtrain_mask], graph\u001b[38;5;241m.\u001b[39my[graph\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     27\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[210], line 11\u001b[0m, in \u001b[0;36mCustomGAT.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m      9\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(x)        \n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[207], line 40\u001b[0m, in \u001b[0;36mCustomGATLayer.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     36\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_v(x)\n\u001b[1;32m     38\u001b[0m head_dim \u001b[38;5;241m=\u001b[39m feature_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\n\u001b[0;32m---> 40\u001b[0m keys    \u001b[38;5;241m=\u001b[39m \u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m queries \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;241m.\u001b[39mview(b, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, head_dim)\n\u001b[1;32m     42\u001b[0m values  \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mview(b, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, head_dim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 2708, 1, 1433]' is invalid for input of size 43328"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=graph.y.unique().shape[0]).to(device)\n",
    "\n",
    "custom_model = CustomGAT().to(device)#CustomGCN().to(device)\n",
    "custom_optim = torch.optim.Adam(custom_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_increase = 0\n",
    "prev_loss = 0\n",
    "\n",
    "def validate(out):\n",
    "    global loss_increase, prev_loss\n",
    "    loss = loss_fn(out[graph.val_mask], graph.y[graph.val_mask])\n",
    "    if loss.item() >= prev_loss:\n",
    "        loss_increase+=1\n",
    "    else:\n",
    "        loss_increase = 0\n",
    "    prev_loss = loss.item()\n",
    "    \n",
    "def train(model, optim):\n",
    "    for epoch in range(200):\n",
    "        if loss_increase == 10:\n",
    "            print(f\"Breaked at {str(epoch)}\")\n",
    "            break\n",
    "        model.train()\n",
    "        out = model(graph)\n",
    "        loss = loss_fn(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        validate(out)\n",
    "        break\n",
    "\n",
    "def test(model):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        logits = model(graph)\n",
    "        preds = torch.softmax(logits, dim = 1).argmax(dim=1)\n",
    "        correct = (preds[graph.test_mask] == graph.y[graph.test_mask]).sum()\n",
    "        print(f\"{accuracy(preds[graph.test_mask], graph.y[graph.test_mask]):.2%}\")\n",
    "\n",
    "train(custom_model, custom_optim)\n",
    "test(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "43f4ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.90%\n"
     ]
    }
   ],
   "source": [
    "train(model, optim)\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbfa4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
